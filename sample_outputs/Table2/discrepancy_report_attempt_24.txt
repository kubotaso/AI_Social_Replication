Here is a point-by-point audit of mismatches between the generated and true results, followed by concrete fixes to reproduce the true table.

Major specification mismatches that drive most of the differences
- Omitted predictors: The generated models exclude Hispanic and Conservative Protestant, both of which appear in the true models.
  - Fix: Add dummy variables for Hispanic and Conservative Protestant (cons_prot) using the same coding as the original. In GSS, Hispanic should be taken from the HISPANIC variable (separate from race) so respondents can be both Black and Hispanic; Conservative Protestant should be coded per the original reltrad/conservative-Protestant classification used in that paper (not simply “Protestant”).
- Dependent variables likely not constructed identically:
  - Model 1 must count dislikes across exactly 6 genres: Rap, Reggae, Blues/R&B, Jazz, Gospel, Latin.
  - Model 2 must count dislikes across the 12 remaining music genres (i.e., all others in the 18-item battery).
  - Fix: Recreate both counts exactly as above. Count “dislike” (and typically “dislike a lot” if a 5-point scale) as 1, other responses as 0; handle “don’t know/haven’t heard” as missing (not 0). Ensure the 6 vs 12 split matches the paper’s list.
- Sample size mismatch:
  - True N: 644 (Model 1) and 605 (Model 2); generated N: 667 and 627.
  - Fix: Use 1993 GSS only; listwise delete on exactly the set of variables used in each model (including Hispanic and cons_prot). That should bring N to 644 and 605.
- Standardization and reporting:
  - The true table reports standardized betas, constants, R2, adjusted R2, N, and significance stars; it does not report SEs.
  - Fix: Report standardized coefficients (beta = b × sd(X)/sd(Y)), using the same sample and variable definitions. It is fine to compute SEs/p-values to assign stars, but do not display SEs if you want the table to match the paper. Use two-tailed tests with thresholds: * p < .05; ** p < .01; *** p < .001.

Model-by-model discrepancies

Model 1 (dislikes among 6 Black/Hispanic-associated genres)
- Variables missing in generated output:
  - Hispanic (true beta: -0.058)
  - Conservative Protestant (true beta: 0.063)
  - Fix: Add both predictors and re-run. Their inclusion will slightly change other betas, R2, stars, and N.
- Standardized coefficients (generated vs true):
  - Racism score: 0.135 vs 0.130; stars: generated *** vs true ** (star mismatch).
  - Education: -0.188 vs -0.175; stars: both ***.
  - Income per capita: -0.034 vs -0.037; stars: both none.
  - Occupational prestige: -0.010 vs -0.020; stars: both none.
  - Female: -0.056 vs -0.057; stars: both none.
  - Age: 0.165 vs 0.163; stars: both ***.
  - Black: -0.119 vs -0.132; stars: generated ** vs true *** (star mismatch).
  - Other race: -0.004 vs -0.017; stars: both none.
  - No religion: 0.033 vs 0.057; stars: both none.
  - Southern: 0.030 vs 0.024; stars: both none.
  - Fix: Including Hispanic and cons_prot and using the exact sample and dependent variable definition should pull the betas and star patterns toward the true values. Also verify standardization formula and that it uses the same sample as the regression.
- Constant (intercept):
  - Generated: 2.469; True: 2.415 (both highly significant).
  - Fix: This should align once you replicate the exact 6-genre count, include the missing predictors, and match the sample (and any mean-centering choices in the original, if any; typically they did not center, so matching the DV and sample is key).
- R2, Adjusted R2, and N:
  - Generated: R2 = 0.137; Adj R2 = 0.124; N = 667.
  - True: R2 = 0.145; Adj R2 = 0.129; N = 644.
  - Fix: Use 1993-only data; listwise delete on the full variable set (including Hispanic, cons_prot); rebuild the DV as specified. That should bring you to R2 ≈ 0.145, Adj R2 ≈ 0.129, N = 644.

Model 2 (dislikes among the other 12 genres)
- Variables missing in generated output:
  - Hispanic (true beta: -0.029)
  - Conservative Protestant (true beta: 0.048)
  - Fix: Add both and re-run.
- Standardized coefficients (generated vs true):
  - Racism score: 0.012 vs 0.080 (magnitude mismatch; both no star).
  - Education: -0.254 vs -0.242; stars: both ***.
  - Income per capita: -0.095 vs -0.065; stars: generated * vs true none (star mismatch).
  - Occupational prestige: 0.042 vs 0.005; stars: both none.
  - Female: -0.056 vs -0.070; stars: both none.
  - Age: 0.083 vs 0.126; stars: generated * vs true ** (star mismatch).
  - Black: 0.118 vs 0.042; stars: generated ** vs true none (star mismatch and magnitude notably larger).
  - Other race: 0.061 vs 0.047; stars: both none.
  - No religion: -0.001 vs 0.024; stars: both none.
  - Southern: 0.091 vs 0.069; stars: generated * vs true none (star mismatch).
  - Fix: The large divergences (especially racism, black, age, income_pc, south) are typical of misspecification from omitted variables (Hispanic, cons_prot), DV mis-definition (not using exactly the “other 12” items or different handling of missing/neutral responses), and sample mismatch. Correct those first; then confirm that the standardized beta computation uses the same sd(y) as in the regression sample.
- Constant (intercept):
  - Generated: 5.597; True: 7.860 (large difference).
  - Fix: This gap is consistent with an incorrect DV (likely not counting the exact 12 genres or a different missing treatment). Rebuild the DV exactly; the intercept should move toward 7.860 when the 12-genre count is correct and the sample matches.
- R2, Adjusted R2, and N:
  - Generated: R2 = 0.138; Adj R2 = 0.124; N = 627.
  - True: R2 = 0.147; Adj R2 = 0.130; N = 605.
  - Fix: As above—redefine the DV correctly, add the omitted predictors, and match the 1993 listwise-deleted sample.

Differences in reporting/interpretation
- Standard errors: The true table does not report SEs; the generated output does. That’s not “wrong,” but it doesn’t match the published table.
  - Fix: Hide SEs and t/p in the table you present if you want an exact visual match; keep them in the model object to assign stars correctly.
- Stars: Several star mismatches appear (e.g., racism and black in Model 1; income_pc, age, black, south in Model 2).
  - Fix: These should align once you correct the model specification and sample. Stars are invariant to standardization, so differences are coming from different samples/DVs/predictor sets or weighting.

Checklist to reproduce the true table
1) Data and sample:
   - Use GSS 1993 only.
   - Listwise delete on all variables used in each model (including Hispanic and Conservative Protestant).
   - Target Ns: 644 (Model 1), 605 (Model 2).

2) Dependent variables:
   - Model 1 DV = count of dislikes across Rap, Reggae, Blues/R&B, Jazz, Gospel, Latin.
   - Model 2 DV = count of dislikes across the remaining 12 music genres.
   - Coding: Count “dislike” categories as 1; treat “don’t know/haven’t heard” as missing for that item.

3) Predictors and coding:
   - Racism score (as in the original data).
   - Education in years (educ).
   - Household income per capita (ensure it matches the original construction: use the same income variable and same household size measure used by the authors).
   - Occupational prestige (prestg80).
   - Female (dummy).
   - Age in years.
   - Black (dummy).
   - Hispanic (dummy from HISPANIC; allow overlap with Black if that matches the paper’s approach).
   - Other race (dummy; White as the omitted race category).
   - Conservative Protestant (dummy; replicate the paper’s classification).
   - No religion (dummy).
   - Southern (dummy).
   - Do not add/remove any other controls.

4) Estimation and standardization:
   - OLS with conventional (non-robust) SEs unless the paper states otherwise.
   - Compute standardized betas as beta_j = b_j × sd(x_j) / sd(y), using the same regression sample for sd(x) and sd(y).
   - Report only standardized betas, significance stars, constant (unstandardized), R2, adjusted R2, and N to match the table.

5) Optional checks that often explain “near misses”:
   - Weights: Verify whether the original analysis used GSS weights. If they did, apply the same weight (e.g., WTSS) consistently. If not, estimate unweighted.
   - Missing data handling for the music items: confirm that non-substantive responses are treated as missing, not “neutral.”
   - Units/scaling: Standardization makes units irrelevant for betas, but intercept and R2 can still differ if DV was built differently.

What you should see after fixes
- Model 1: Standardized betas very close to the true list (racism ≈ 0.130 [**], educ ≈ -0.175 [***], age ≈ 0.163 [***], black ≈ -0.132 [***], etc.), constant ≈ 2.415, R2 ≈ 0.145, Adj R2 ≈ 0.129, N = 644.
- Model 2: Standardized betas very close to the true list (racism ≈ 0.080, educ ≈ -0.242 [***], age ≈ 0.126 [**], black ≈ 0.042, etc.), constant ≈ 7.860, R2 ≈ 0.147, Adj R2 ≈ 0.130, N = 605.

If, after making all changes above, small residual differences remain (on the order of 0.005–0.015 in some betas), check:
- That the Hispanic indicator is constructed independently of race (not folded into “other race”).
- That Conservative Protestant matches the paper’s recode exactly.
- That the 6 vs 12 genre lists exactly match the paper’s composition and item handling.